{"cells":[{"source":"# Web Scraping & NPL for Novel Word Frequency","metadata":{},"cell_type":"markdown","id":"220f5137-feee-4bcc-be9b-fcc444c1598d"},{"source":"<p>In this project, I will find out what are the most frequent words in Herman Melville's novel, Moby Dick, and how often do they occur? Firstly,I will use requests and BeautifulSoup to scrape a novel from the Project Gutenberg website. After scraping and cleaning the text data, you will use NLP to find the most frequent words in Moby Dick. <p>\n","metadata":{},"cell_type":"markdown","id":"774b7c71-834a-446e-b974-55cab4878557"},{"source":"## 1. Import packages","metadata":{"dc":{"key":"3"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"id":"7357105a-6bfe-4ba0-be28-6c13bf253048","cell_type":"markdown"},{"source":"# Importing requests, BeautifulSoup, nltk, and Counter\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nnltk.download('stopwords')\nfrom collections import Counter","metadata":{"dc":{"key":"3"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"executionTime":95,"lastSuccessfullyExecutedCode":"# Importing requests, BeautifulSoup, nltk, and Counter\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nnltk.download('stopwords')\nfrom collections import Counter"},"id":"6b2f24bd-f234-45df-8c4f-d9b892e0f7e1","cell_type":"code","execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}]},{"source":"## 2. Web Scraping for text in Moby Dick ","metadata":{"dc":{"key":"10"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"id":"5e7bf695-1ddc-45fd-bddb-0f11f3508964","cell_type":"markdown"},{"source":"# Extracting characters from Moby Dick HTML \nr = requests.get('https://www.gutenberg.org/files/2701/2701-h/2701-h.htm')\nr.encoding = 'utf-8'\nhtml = r.text\nprint(html[0:2000])","metadata":{"dc":{"key":"10"},"tags":["sample_code"],"trusted":true,"executionTime":204,"lastSuccessfullyExecutedCode":"# Extracting characters from Moby Dick HTML \nr = requests.get('https://www.gutenberg.org/files/2701/2701-h/2701-h.htm')\nr.encoding = 'utf-8'\nhtml = r.text\nprint(html[0:2000])"},"id":"70e1d951-5fa8-4b80-afdb-faa1b02cdad5","cell_type":"code","execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\n<meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n<title>The Project Gutenberg eBook of Moby Dick; Or the Whale, by Herman Melville</title>\n\n<style type=\"text/css\" xml:space=\"preserve\">\n\n    body {margin-left:15%; margin-right:15%; text-align:justify }\n    p { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\n    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\n    hr  { width: 50%; text-align: center;}\n    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\n    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\n    .toc       { margin-left: 10%; margin-bottom: .75em;}\n    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\n\n    table      {margin-left: 10%;}\n\na:link {color:blue;\n\t\ttext-decoration:none}\nlink {color:blue;\n\t\ttext-decoration:none}\na:visited {color:blue;\n\t\ttext-decoration:none}\na:hover {color:red}\n\n</style>\n  </head>\n  <body>\n\n<div style='text-align:center; font-size:1.2em; font-weight:bold;'>The Project Gutenberg eBook of Moby-Dick; or The Whale, by Herman Melville</div>\n<div style='display:block; margin:1em 0'>\nThis eBook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this eBook or online\nat <a href=\"https://www.gutenberg.org\">www.gutenberg.org</a>. If you\nare not located in the United States, you will have to check the laws of the\ncountry where you are located before using this eBoo\n"}]},{"source":"# Getting text by using BeautifulSoup \nsoup = BeautifulSoup(html,\"html.parser\")\ntext =soup.get_text()\nprint(text[32000:34000])","metadata":{"dc":{"key":"17"},"tags":["sample_code"],"trusted":true,"executionTime":171,"lastSuccessfullyExecutedCode":"# Getting text by using BeautifulSoup \nsoup = BeautifulSoup(html,\"html.parser\")\ntext =soup.get_text()\nprint(text[32000:34000])"},"id":"e277adc9-3658-41a3-8bbf-bd01f8a7cb79","cell_type":"code","execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":"inging up the rear\n      of every funeral I meet; and especially whenever my hypos get such an\n      upper hand of me, that it requires a strong moral principle to prevent me\n      from deliberately stepping into the street, and methodically knocking\n      people’s hats off—then, I account it high time to get to sea as soon\n      as I can. This is my substitute for pistol and ball. With a philosophical\n      flourish Cato throws himself upon his sword; I quietly take to the ship.\n      There is nothing surprising in this. If they but knew it, almost all men\n      in their degree, some time or other, cherish very nearly the same feelings\n      towards the ocean with me.\n    \n\n      There now is your insular city of the Manhattoes, belted round by wharves\n      as Indian isles by coral reefs—commerce surrounds it with her surf.\n      Right and left, the streets take you waterward. Its extreme downtown is\n      the battery, where that noble mole is washed by waves, and cooled by\n      breezes, which a few hours previous were out of sight of land. Look at the\n      crowds of water-gazers there.\n    \n\n      Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears\n      Hook to Coenties Slip, and from thence, by Whitehall, northward. What do\n      you see?—Posted like silent sentinels all around the town, stand\n      thousands upon thousands of mortal men fixed in ocean reveries. Some\n      leaning against the spiles; some seated upon the pier-heads; some looking\n      over the bulwarks of ships from China; some high aloft in the rigging, as\n      if striving to get a still better seaward peep. But these are all\n      landsmen; of week days pent up in lath and plaster—tied to counters,\n      nailed to benches, clinched to desks. How then is this? Are the green\n      fields gone? What do they here?\n    \n\n      But look! here come more crowds, pacing straight for the water, and\n      seemingly bound for a dive. Strange! Nothing w\n"}]},{"source":"## 3. Extract and process words via Natural Language Toolkit","metadata":{"dc":{"key":"24"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"id":"c05e13e1-6394-468e-ae7c-bb00dd04c519","cell_type":"markdown"},{"source":"# Tokenizing the text\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\ntokens = tokenizer.tokenize(text) \ntokens[0:8]","metadata":{"dc":{"key":"24"},"tags":["sample_code"],"trusted":true,"executionTime":86,"lastSuccessfullyExecutedCode":"# Tokenizing the text\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\ntokens = tokenizer.tokenize(text) \ntokens[0:8]"},"id":"cdd5ff4c-7332-437d-be0e-3042f3d51379","cell_type":"code","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"['The', 'Project', 'Gutenberg', 'eBook', 'of', 'Moby', 'Dick', 'Or']"},"metadata":{}}]},{"source":"# Make the words lowercase\nwords=[token.lower() for token in tokens]\nwords[:8]","metadata":{"dc":{"key":"31"},"tags":["sample_code"],"trusted":true,"executionTime":90,"lastSuccessfullyExecutedCode":"# Make the words lowercase\nwords=[token.lower() for token in tokens]\nwords[:8]"},"id":"7b5efe9a-03e2-4e1f-9974-12ccba5a915b","cell_type":"code","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"['the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or']"},"metadata":{}}]},{"source":"# Getting the English stop words from nltk\nsw = nltk.corpus.stopwords.words('english')\n\n# Printing out the first eight stop words\nsw[:8]","metadata":{"dc":{"key":"38"},"tags":["sample_code"],"trusted":true,"executionTime":83,"lastSuccessfullyExecutedCode":"# Getting the English stop words from nltk\nsw = nltk.corpus.stopwords.words('english')\n\n# Printing out the first eight stop words\nsw[:8]"},"id":"ecc3006b-f171-439d-85b1-ecf1e0cc5ca5","cell_type":"code","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']"},"metadata":{}}]},{"source":"## 4. Remove stop words in Moby Dick\n<p>We now want to create a new list with all <code>words</code> in Moby Dick, except those that are stop words (that is, those words listed in <code>sw</code>).</p>","metadata":{"dc":{"key":"45"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"id":"919ed08d-76a3-40ad-9a6c-8e357227da46","cell_type":"markdown"},{"source":"# Remove stop words in Moby Dick\nwords_ns=[word for word in words if word not in sw]\nwords_ns[:5]","metadata":{"dc":{"key":"45"},"tags":["sample_code"],"trusted":true,"executionTime":100,"lastSuccessfullyExecutedCode":"# Remove stop words in Moby Dick\nwords_ns=[word for word in words if word not in sw]\nwords_ns[:5]"},"id":"5bb24f5a-e698-4c44-b072-8c9a6202e169","cell_type":"code","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"['project', 'gutenberg', 'ebook', 'moby', 'dick']"},"metadata":{}}]},{"source":"## 5. The most common word","metadata":{"dc":{"key":"52"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"id":"d9071ab7-9a07-4625-a03d-164b7593a168","cell_type":"markdown"},{"source":"# Count words and store 10 most common words\ncount = Counter(words_ns)\ntop_ten =count.most_common(10)\nprint(top_ten)","metadata":{"dc":{"key":"52"},"tags":["sample_code"],"trusted":true,"executionTime":73,"lastSuccessfullyExecutedCode":"# Count words and store 10 most common words\ncount = Counter(words_ns)\ntop_ten =count.most_common(10)\nprint(top_ten)"},"id":"22b895e2-dc73-4831-acc8-02f60a52048c","cell_type":"code","execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":"[('whale', 1244), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}